
# 睿抗算法调优赛道 国一方案（遥感图像分类）

## 📄 项目描述
本项目使用PyTorch框架和EfficientNet-B7模型，实现遥感图像的五分类任务（飞机、桥梁、宫殿、船舶、体育场）。包含完整的数据预处理、数据增强、模型训练和保存流程。本项目已开源，具体代码请访问：[GitHub 项目链接](https://github.com/123pyLeo/RAICOM_RS/tree/main)

## 📁 数据集准备
### 数据集结构
```
root/
├── datasets/
│   ├── airplane/
│   ├── bridge/
│   ├── palace/
│   ├── ship/
│   └── stadium/
└── train.ipynb
```

### 数据划分
- 自动分割训练集/测试集（90%/10%）
- 保持类别分布均衡
- 随机种子固定（random_state=42）

## ⚙️ 环境要求
### 依赖库
```bash
pip install torch torchvision scikit-learn pillow tqdm
```

### 硬件建议
- 3060以上显卡
- 至少16GB内存（训练时）

## 🏋️ 数据增强策略
| 增强类型                | 参数设置                     |
|-----------------------|---------------------------|
| 随机裁剪                | 224x224 (缩放0.8-1.0)       |
| 颜色抖动                | 亮度/对比度/饱和度=0.4，色相=0.2 |
| 仿射变换                | 平移10%，缩放0.8-1.2，剪切10°  |
| 高斯模糊                | 内核5x5，σ=0.1-2.0          |
| 随机擦除                | 概率50%，面积2%-40%         |

## 🏗️ 模型配置
### 网络架构
- 基于EfficientNet-B7预训练模型
- 修改最后一层全连接层：
  ```python
  model.classifier[1] = nn.Linear(num_ftrs, 5)
  ```

### 训练参数
| 参数                 | 设置        |
|---------------------|-----------|
| 损失函数              | 交叉熵 + 标签平滑（α=0.9） |
| 优化器               | Adam (lr=0.001) |
| 学习率调度            | StepLR (step=7, γ=0.1) |
| Batch Size          | 32        |
| Epochs              | 25        |

## 📝 使用方法
1. 准备数据集（按上述结构组织）
2. 运行Jupyter训练脚本
3. 训练完成后会生成模型文件

## ⚠️ 注意事项
1. 首次运行会自动创建训练集/测试集目录
2. 修改`data_dir`路径需同步调整数据集结构
3. 随机增强参数可能导致不同设备间结果微小差异
4. 完整训练需要约5GB GPU显存

## ⚗️ 关于蒸馏
- 在比赛过程中，由于需要在平台上运行程序并通过接口测试，因此考虑通过蒸馏来实现小权重，节省本地上传时间。
- b0蒸馏出来的效果不如b7效果好，针对比赛，性能优先，所以最后放弃蒸馏方案。
- 若后续仍希望尝试蒸馏方案以提高`EfficientNet-B0`的性能，使其更接近`EfficientNet-B7`，可尝试以下改进措施：

### （一）调整蒸馏参数
蒸馏损失函数中的温度参数`temperature`和权重参数`alpha`对蒸馏效果有重要影响。
```python
def distillation_loss(student_logits, teacher_logits, labels, temperature=4.0, alpha=0.5):
    soft_loss = F.kl_div(F.log_softmax(student_logits / temperature, dim=1),
                         F.softmax(teacher_logits / temperature, dim=1),
                         reduction='batchmean') * (temperature ** 2)
    hard_loss = cross_entropy_loss(student_logits, labels)
    return alpha * hard_loss + (1 - alpha) * soft_loss
```

### （二）增加训练轮数
适当增加训练轮数，让学生模型有更多的时间学习教师模型的知识。由于学生模型需要学习教师模型的知识，特别是在模型架构差异较大的情况下，可能需要更多的训练轮数来收敛。
```python
# 增加训练轮数
num_epochs = 50

# 训练学生模型
student_model = train_model(teacher_model, student_model, distillation_loss, optimizer, scheduler, num_epochs=num_epochs)
```

### （三）调整学习率
学习率是影响模型训练的重要因素。尝试不同的学习率和学习率调度策略，如使用`CosineAnnealingLR`，可以提高模型的收敛速度和性能。
```python
from torch.optim.lr_scheduler import CosineAnnealingLR
optimizer = optim.Adam(student_model.parameters(), lr=0.001)
scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)
```

## 📚 参考文献
- [PyTorch官方文档](https://pytorch.org/docs/stable/index.html)
- [EfficientNet论文](https://arxiv.org/abs/1905.11946)
- [Label Smoothing研究](https://arxiv.org/abs/1512.00567)
